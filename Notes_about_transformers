#######################
Based on the experiments with BERT-F1-score in translation and summarization, BERT-F1-score of even 53 (or 0.53) doesn't mean that the predicted sentences are related to the context of the input sentences so the minimum value bar for BERT-score should be atleast greater than 55. Test few samples with bert-score to get an idea of how it works.
  
Not implemented features:-
  Beam_search doesn't output attention weights, if required then need to modify the core t2t algorithm
  Multiple occurences of stop values in beam search and repeated occurences of n-grams
  Mixed precision is disabled with bertified_transformer architecture

Watchout for:-
  Source code needs to be changed once the below changes are implemented by the community
  a) Gradient accumulation implementation by tensorflow community
  b) label smoothing fix.

For using custom tokenizers for transformer model please train it using 'create_tokenizer_using_huggingface_tokenizers.py'
Please donot train BERTWordpiece tokenizer for tamil seems it removes '.' from the characters make use of ByteLevelBPETokenizer
instead.
  
#########################
Positional encoding:-
	'''Positional encoding is added to give the model some information about the 
   relative position of the words in the sentence.Nearby tokens will have 
   similar position-encoding vectors. Any relative position encoding can be written 
   as a linear function of the current position. Raw angles are not a good model 
   input (they're either unbounded, or discontinuous) so take the sine and cosine'''

dec_padding_mask:-
	# Used in the 2nd attention block in the decoder.
    # Used to mask the encoder outputs.

dec_target_padding_mask:-    
    # Used in the 1st attention block in the decoder.
    # It is used to pad and mask future tokens in the input received by 
    # the decoder.Allows the decoder to attend to all positions in the decoder up to and 
    # including that position(refer architecture)

scaled_dot_product_attention:-
	"""Calculate the attention weights.
    q, k, v must have matching leading dimensions.
    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.
    The mask has different shapes depending on its type(padding or look ahead) 
    but it must be broadcastable for addition.
    
    Args:
      q: query shape == (..., seq_len_q, depth)
      k: key shape == (..., seq_len_k, depth)
      v: value shape == (..., seq_len_v, depth_v)
      mask: Float tensor with shape broadcastable 
            to (..., seq_len_q, seq_len_k). Defaults to None.
      
    Returns:
      output, attention_weights
    """

MultiHeadAttention:-

		The scaled_dot_product_attention defined above is applied to each head (broadcasted for efficiency). 
		An appropriate mask must be used in the attention step. The attention output for each head is then concatenated 
		and put through a final Dense layer.Instead of one single attention head, Q, K, and V are split into multiple 
		heads because it allows the model to jointly attend to information at different positions from different 
		representational spaces. After the split each head has a reduced dimensionality, 
		so the total computation cost is the same as a single head attention with full dimensionality.