#########################
Positional encoding:-
	'''Positional encoding is added to give the model some information about the 
   relative position of the words in the sentence.Nearby tokens will have 
   similar position-encoding vectors. Any relative position encoding can be written 
   as a linear function of the current position. Raw angles are not a good model 
   input (they're either unbounded, or discontinuous) so take the sine and cosine'''

dec_padding_mask:-
	# Used in the 2nd attention block in the decoder.
    # Used to mask the encoder outputs.

dec_target_padding_mask:-    
    # Used in the 1st attention block in the decoder.
    # It is used to pad and mask future tokens in the input received by 
    # the decoder.Allows the decoder to attend to all positions in the decoder up to and 
    # including that position(refer architecture)

scaled_dot_product_attention:-
	"""Calculate the attention weights.
    q, k, v must have matching leading dimensions.
    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.
    The mask has different shapes depending on its type(padding or look ahead) 
    but it must be broadcastable for addition.
    
    Args:
      q: query shape == (..., seq_len_q, depth)
      k: key shape == (..., seq_len_k, depth)
      v: value shape == (..., seq_len_v, depth_v)
      mask: Float tensor with shape broadcastable 
            to (..., seq_len_q, seq_len_k). Defaults to None.
      
    Returns:
      output, attention_weights
    """

MultiHeadAttention:-

		The scaled_dot_product_attention defined above is applied to each head (broadcasted for efficiency). 
		An appropriate mask must be used in the attention step. The attention output for each head is then concatenated 
		and put through a final Dense layer.Instead of one single attention head, Q, K, and V are split into multiple 
		heads because it allows the model to jointly attend to information at different positions from different 
		representational spaces. After the split each head has a reduced dimensionality, 
		so the total computation cost is the same as a single head attention with full dimensionality.