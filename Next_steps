Why ? :- http://www.unesco.org/new/fileadmin/MULTIMEDIA/HQ/CLT/pdf/FlyerEndangeredLanguages-WebVersion.pdf
###################################################
Packages in colab + Additonal packages
	*) langdetect
	*) Rouge score
	*) bert score
	*) clone bleu score repo (models/official/google)
    *) tensor2tensor
################################
Papers to read:-
	https://arxiv.org/abs/1904.07418 :- Positional Encoding to Control Output Sequence Length
	https://arxiv.org/pdf/1905.05475.pdf :- Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies	
####################################################
Addtional refine mechanism:-

http://cs229.stanford.edu/proj2019aut/data/assignment_308832_raw/26632588.pdf:- Policy gradient approach to refine.
https://arxiv.org/abs/1910.10683 :- T5
https://arxiv.org/abs/2003.10555:- electra
https://kentonl.com/pub/gltpc.2020.pdf :- salient span masking, focus on problems that require world knowledge (suits QA)


Enhance neural network understanding:-

https://arxiv.org/pdf/2004.04696.pdf :- BLEURT: Learning Robust Metrics for Text Generation
https://arxiv.org/abs/1809.08895 :- text to speech transformer
								(https://colab.research.google.com/github/as-ideas/TransformerTTS/blob/master/notebooks/synthesize.ipynb)
https://arxiv.org/pdf/2004.10188.pdf :- energy based models for text 
https://www.microsoft.com/en-us/research/uploads/prod/2018/02/UniNMT.pdf :- shared vocab bw souce and target sentences
https://www-nlp.stanford.edu/pubs/clark2019what.pdf :- analyse BERT's attention
https://arxiv.org/pdf/1906.05909.pdf :- Self-attention in vision
https://arxiv.org/abs/1911.03584 :- relation bw attention and convolution
https://arxiv.org/abs/2004.14546 :- train the model to output the explanation after generating the (natural text) prediction.
https://colinraffel.com/publications/arxiv2020how.pdf :- self-supervised training T5 for QA

Optimize the model:-
	https://www.aaai.org/ojs/index.php/AAAI/article/view/4487 :- Tied transformers:parameter sharing bw encoder and decoder
	https://arxiv.org/abs/2001.04451 :- Reformer
	
	*) online Data augumentation  during training
		https://colab.research.google.com/drive/1RGWrQv3e0CRDPDROQ3ZmUWTmlRljasGi?authuser=0#scrollTo=E9RYnn9VDE4N
    *) Remove Tamil blacklist from the preprocess code
####################################################
Coding
 
 clean the download and encode script
 b) then move it to bucket, replace set core_path in config to gs_bucket path
 c) train with TPU
 d) padding test
train the model in GCP
*) tf profile optimization
*) stage wise testing
*) clean summarize_or_translate repo
*) Rewards for RL approach
			a) immediate reward:- autoregressive score using decoder.. like beam search
			b) BERT score :- reward to use once the sentence is generated
			c) An ensembled reward function as a linear
				combination(use linear regression on annotated data to learn the linear weights) 
				of ROUGE, BLEU, BERTScore, etc. that better approximates human evaluation; 		
#####################################################
Addtional refine mechanism:-
		a) Electra style used for NLU..not sure whether it could be used for NLG.
		b) https://arxiv.org/pdf/1911.03829.pdf :- make the generated output coherent by establishing a global context before 
				generating tokens 
		c) Mixmatch, Fixmatch, Remixmatch
		d) UDA
		e) self-training
		f) contrastive predictive coding
###################################################
#) automatically suggest tokens per batch based on the available GPU memory
#) source_text == decode(encode(source_text))

#) functions should be small and should be specific
#) avoid composite switch statements
#) remove unused python packages
#) one line space after function name and before return
#) check duplicate code
###################################################
Expected improvements

	*) XLM ROBERTA as the multilingual pretrained model:- https://github.com/pytorch/fairseq/tree/master/examples/xlmr
			https://huggingface.co/transformers/model_doc/xlmroberta.html
	*) Add electra multilingual pretrained model when they are out
	*) Adafactor optimizer:- consumes less auxiliary storage
    *) Collect all indian languages from that link
###################################################
Additional enhancements

	#) Tensorflow profiler
	#) Tensorflow graph optimizations
	#) Hyper parameter tuning using keras tuner
		Randomsearch using keras-tuner https://keras-team.github.io/keras-tuner/tutorials/subclass-tuner/
	#) Support distributed training, 
		*)ship the dataset to a bucket
    #) Scheduled sampling :- toss a coin and perform teacher forcing if it is a heads else perform teacher_force training 
    #) Avg checkpoints
	